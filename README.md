# **Indian Sign Language Gesture Recognition To Multilingual Text**  

A **real-time gesture recognition system** that converts **hand movements into text translations** in **English, Hindi, and Gujarati**. This project bridges communication gaps by allowing users to express words through simple hand gestures, which are recognized using **deep learning and computer vision**.  

---

## **ğŸ“Œ Project Overview**  

Sign languages and gesture-based communication are often underutilized due to a lack of widespread tools for recognition. This project uses a **long short-term memory (LSTM) model** trained on hand gesture sequences to recognize gestures and display **real-time translations**. The system is designed to be lightweight and efficient for real-world applications, with potential uses in accessibility tools, education, and human-computer interaction.  

### **Key Features**  

âœ… **Real-time hand tracking** using **MediaPipe Hands**  
âœ… **Sequential gesture recognition** powered by an **LSTM model**  
âœ… **Multi-language support** (English, Hindi, Gujarati)  
âœ… **Optimized prediction smoothing** to improve recognition stability  
âœ… **Custom font rendering** for better visibility in different languages  
âœ… **Supports one-hand and two-hand gestures**  

---

## **ğŸ“ Folder Structure**  

```
/gesture-translator
â”‚â”€â”€ /fonts                           # Font files for Hindi & Gujarati
â”‚â”€â”€ /data_preprocessing               # Preprocessing scripts & label mappings
â”‚â”€â”€ dataset_collection.py             # Collects and processes gesture data
â”‚â”€â”€ preprocess_data.py                 # Extracts hand keypoints for training
â”‚â”€â”€ train_model.py                     # Trains LSTM model for gesture recognition
â”‚â”€â”€ real_time_translation.py           # Real-time inference with translations
â”‚â”€â”€ lstm_model.h5                      # Trained LSTM model
â”‚â”€â”€ label_mapping.pickle               # Stores class labels & translations
â”‚â”€â”€ README.md                          # Project documentation
```

---

## **ğŸ› ï¸ Setup & Installation**  

Before running the project, install the required dependencies:  

```bash
pip install opencv-python mediapipe numpy tensorflow keras pillow
```

---

## **ğŸ“Š Data Collection & Processing**  

### **1ï¸âƒ£ Collect Gesture Data**  
This script captures **video sequences** of hand gestures and extracts **MediaPipe hand keypoints**.  

```bash
python dataset_collection.py
```

- The system captures **30 frames per gesture** to ensure smooth recognition.  
- Extracted keypoints are **normalized and stored** for training.  

### **2ï¸âƒ£ Preprocess the Data**  
Convert raw keypoints into a structured dataset for training:  

```bash
python preprocess_data.py
```

- Ensures **consistent data format**.  
- Saves extracted features for training the LSTM model.  

---

## **ğŸ“ˆ Training the Model**  

### **3ï¸âƒ£ Train the LSTM Model**  
```bash
python train_model.py
```
- Uses a **sequential LSTM network** to learn gesture patterns.  
- The trained model is saved as **`lstm_model.h5`**.  

---

## **ğŸ¥ Running Real-Time Gesture Translation**  

### **4ï¸âƒ£ Start Gesture Recognition**  
```bash
python real_time_translation.py
```
- Uses the webcam to detect **hand movements**.  
- Displays **live translations** in **English, Hindi, and Gujarati**.  

**Exit by pressing `q`.**  

---

## **ğŸ–¥ï¸ How It Works?**  

1ï¸âƒ£ **Detects hand movements** using MediaPipe.  
2ï¸âƒ£ **Extracts keypoints** and converts them into a time-sequenced dataset.  
3ï¸âƒ£ **Feeds the data into an LSTM model** for classification.  
4ï¸âƒ£ **Predicts the gesture and displays translations** in **three languages**.  

ğŸ’¡ **To improve accuracy, predictions are smoothed using history tracking.**  

---

## **ğŸ¨ Customization**  

### **Modify Font Colors or Sizes**  
Edit `real_time_translation.py`:  

```python
hindi_font = ImageFont.truetype("fonts/NotoSansDevanagari-VariableFont_wdth,wght.ttf", 42)
gujarati_font = ImageFont.truetype("fonts/NotoSansGujarati-VariableFont_wdth,wght.ttf", 42)
text_colors = {"english": (255, 255, 255), "hindi": (255, 255, 0), "gujarati": (0, 255, 255)}
```

### **Adjust Prediction Confidence**  
Change the minimum confidence required for displaying a prediction:  

```python
CONFIDENCE_THRESHOLD = 0.85
```

---

## **ğŸ”¹ Future Enhancements**  

ğŸ”¸ Expand the dataset for more **gestures and words**.  
ğŸ”¸ Improve detection for **partial hand visibility**.  
ğŸ”¸ Add **voice output** for translations.  
ğŸ”¸ Support additional **Indian languages** (e.g., Tamil, Bengali, Marathi).  

---

## **ğŸ‘¨â€ğŸ’» Contributors**  

- **Harshad Nagpure** â€“ Developed & Trained the Model  
- **Dewansh Gopani** â€“ Assisted with Data Collection & Fine Tuning   

ğŸ“§ **Contact:** h.incworks@gmail.com  

---

## **ğŸ”— References**  

- **MediaPipe Hands**: [https://developers.google.com/mediapipe](https://developers.google.com/mediapipe)  
- **LSTM for Gesture Recognition**: [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)  
- **ISL Gestures & Dictionary** : [https://islrtc.nic.in/]

---
